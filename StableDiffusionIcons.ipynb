{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtQiyvCgyeDK"
   },
   "source": [
    "# Stable Diffusion Icons: Fine-Tune Training of Stable Diffusion\n",
    "\n",
    "Based on: [Pokemon FineTuning from Lambda Labs ML](https://github.com/LambdaLabsML/examples/blob/main/stable-diffusion-finetuning/pokemon_finetune.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5QbrUN7E37TI",
    "outputId": "f3ec23fa-52c1-4952-cc8f-2d70d506c234"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Obtaining taming-transformers from git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers (from -r requirements.txt (line 24))\n",
      "  Cloning https://github.com/CompVis/taming-transformers.git (to revision master) to ./src/taming-transformers\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/CompVis/taming-transformers.git /home/ubuntu/stable-diffusion/src/taming-transformers\n",
      "  Resolved https://github.com/CompVis/taming-transformers.git to commit 24268930bf1dce879235a7fddd0b2355b84d7ea6\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hObtaining clip from git+https://github.com/openai/CLIP.git@main#egg=clip (from -r requirements.txt (line 25))\n",
      "  Cloning https://github.com/openai/CLIP.git (to revision main) to ./src/clip\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /home/ubuntu/stable-diffusion/src/clip\n",
      "  Resolved https://github.com/openai/CLIP.git to commit d50d76daa670286dd6cacf3bcd80b5e4823fc8e1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hObtaining file:///home/ubuntu/stable-diffusion (from -r requirements.txt (line 26))\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torch==1.12.1\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.12.1%2Bcu113-cp38-cp38-linux_x86_64.whl (1837.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.13.1\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.13.1%2Bcu113-cp38-cp38-linux_x86_64.whl (23.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.4/23.4 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting albumentations==0.4.3\n",
      "  Downloading albumentations-0.4.3.tar.gz (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting opencv-python==4.5.5.64\n",
      "  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pudb==2019.2\n",
      "  Downloading pudb-2019.2.tar.gz (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting imageio==2.9.0\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting imageio-ffmpeg==0.4.2\n",
      "  Downloading imageio_ffmpeg-0.4.2-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytorch-lightning==1.4.2\n",
      "  Downloading pytorch_lightning-1.4.2-py3-none-any.whl (916 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m916.6/916.6 kB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting omegaconf==2.1.1\n",
      "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting test-tube>=0.7.5\n",
      "  Downloading test_tube-0.7.5.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting streamlit>=0.73.1\n",
      "  Downloading streamlit-1.13.0-py2.py3-none-any.whl (9.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.2/9.2 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops==0.3.0\n",
      "  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
      "Collecting torch-fidelity==0.3.0\n",
      "  Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
      "Collecting transformers==4.22.2\n",
      "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m128.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kornia==0.6\n",
      "  Downloading kornia-0.6.0-py2.py3-none-any.whl (367 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting webdataset==0.2.5\n",
      "  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchmetrics==0.6.0\n",
      "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.4/329.4 kB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fire==0.4.0\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio==3.1.4\n",
      "  Downloading gradio-3.1.4-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting diffusers==0.3.0\n",
      "  Downloading diffusers-0.3.0-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets[vision]==2.4.0\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.8/site-packages (from torch==1.12.1->-r requirements.txt (line 2)) (4.3.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.8/site-packages (from torchvision==0.13.1->-r requirements.txt (line 3)) (1.23.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision==0.13.1->-r requirements.txt (line 3)) (7.0.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.8/site-packages (from torchvision==0.13.1->-r requirements.txt (line 3)) (2.28.1)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from albumentations==0.4.3->-r requirements.txt (line 5)) (5.3.1)\n",
      "Collecting imgaug<0.2.7,>=0.2.5\n",
      "  Downloading imgaug-0.2.6.tar.gz (631 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.4/631.4 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /home/ubuntu/.local/lib/python3.8/site-packages (from albumentations==0.4.3->-r requirements.txt (line 5)) (1.9.1)\n",
      "Requirement already satisfied: pygments>=1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from pudb==2019.2->-r requirements.txt (line 7)) (2.13.0)\n",
      "Collecting urwid>=1.1.1\n",
      "  Downloading urwid-2.1.2.tar.gz (634 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.6/634.6 kB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from pytorch-lightning==1.4.2->-r requirements.txt (line 10)) (21.3)\n",
      "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
      "  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from pytorch-lightning==1.4.2->-r requirements.txt (line 10)) (4.64.1)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /usr/lib/python3/dist-packages (from pytorch-lightning==1.4.2->-r requirements.txt (line 10)) (2.9.1)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/lib/python3/dist-packages (from pytorch-lightning==1.4.2->-r requirements.txt (line 10)) (0.18.2)\n",
      "Collecting antlr4-python3-runtime==4.8\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.9.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers==4.22.2->-r requirements.txt (line 16)) (3.0.12)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2022.9.13-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting braceexpand\n",
      "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire==0.4.0->-r requirements.txt (line 20)) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/lib/python3/dist-packages (from fire==0.4.0->-r requirements.txt (line 20)) (1.1.0)\n",
      "Requirement already satisfied: Jinja2 in /home/ubuntu/.local/lib/python3.8/site-packages (from gradio==3.1.4->-r requirements.txt (line 21)) (3.1.2)\n",
      "Collecting python-multipart\n",
      "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting analytics-python\n",
      "  Downloading analytics_python-1.4.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.8/site-packages (from gradio==3.1.4->-r requirements.txt (line 21)) (3.5.3)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h11<0.13,>=0.11\n",
      "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi\n",
      "  Downloading fastapi-0.85.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson\n",
      "  Downloading orjson-3.8.0-cp38-cp38-manylinux_2_28_x86_64.whl (145 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.9/145.9 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic in /home/ubuntu/.local/lib/python3.8/site-packages (from gradio==3.1.4->-r requirements.txt (line 21)) (1.9.2)\n",
      "Requirement already satisfied: fsspec in /usr/lib/python3/dist-packages (from gradio==3.1.4->-r requirements.txt (line 21)) (0.6.1)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.19.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.8/site-packages (from gradio==3.1.4->-r requirements.txt (line 21)) (1.4.4)\n",
      "Collecting pycryptodome\n",
      "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx\n",
      "  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting paramiko\n",
      "  Downloading paramiko-2.11.0-py2.py3-none-any.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting markdown-it-py[linkify,plugins]\n",
      "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/ubuntu/.local/lib/python3.8/site-packages (from diffusers==0.3.0->-r requirements.txt (line 22)) (4.12.0)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-9.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.9/212.9 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tornado>=5.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit>=0.73.1->-r requirements.txt (line 13)) (6.2)\n",
      "Collecting tzlocal>=1.1\n",
      "  Downloading tzlocal-4.2-py3-none-any.whl (19 kB)\n",
      "Collecting watchdog\n",
      "  Downloading watchdog-2.1.9-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting validators>=0.2\n",
      "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pydeck>=0.1.dev5\n",
      "  Downloading pydeck-0.8.0b4-py2.py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/lib/python3/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 13)) (7.0)\n",
      "Collecting pympler>=0.9\n",
      "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit>=0.73.1->-r requirements.txt (line 13)) (2.8.2)\n",
      "Collecting rich>=10.11.0\n",
      "  Downloading rich-12.6.0-py3-none-any.whl (237 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.5/237.5 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: blinker>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 13)) (1.4)\n",
      "Collecting altair>=3.2.0\n",
      "  Downloading altair-4.2.0-py3-none-any.whl (812 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/lib/python3/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 13)) (4.0.0)\n",
      "Collecting semver\n",
      "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gitpython!=3.1.19\n",
      "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.5/182.5 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=3.20.2,<4,>=3.12\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: toolz in /usr/lib/python3/dist-packages (from altair>=3.2.0->streamlit>=0.73.1->-r requirements.txt (line 13)) (0.9.0)\n",
      "Requirement already satisfied: entrypoints in /usr/lib/python3/dist-packages (from altair>=3.2.0->streamlit>=0.73.1->-r requirements.txt (line 13)) (0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/lib/python3/dist-packages (from altair>=3.2.0->streamlit>=0.73.1->-r requirements.txt (line 13)) (3.2.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->gradio==3.1.4->-r requirements.txt (line 21)) (19.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->gradio==3.1.4->-r requirements.txt (line 21)) (2.1.1)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (262 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.1/262.1 kB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-image>=0.11.0 in /usr/lib/python3/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.3->-r requirements.txt (line 5)) (0.16.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata->diffusers==0.3.0->-r requirements.txt (line 22)) (1.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=17.0->pytorch-lightning==1.4.2->-r requirements.txt (line 10)) (2.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->gradio==3.1.4->-r requirements.txt (line 21)) (2022.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from Jinja2->gradio==3.1.4->-r requirements.txt (line 21)) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchvision==0.13.1->-r requirements.txt (line 3)) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision==0.13.1->-r requirements.txt (line 3)) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision==0.13.1->-r requirements.txt (line 3)) (2.8)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting commonmark<0.10.0,>=0.9.0\n",
      "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting backports.zoneinfo\n",
      "  Downloading backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_x86_64.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytz-deprecation-shim\n",
      "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /usr/lib/python3/dist-packages (from validators>=0.2->streamlit>=0.73.1->-r requirements.txt (line 13)) (4.4.2)\n",
      "Collecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting backoff==1.10.0\n",
      "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting starlette==0.20.4\n",
      "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from starlette==0.20.4->fastapi->gradio==3.1.4->-r requirements.txt (line 21)) (3.6.1)\n",
      "Collecting wcwidth>=0.2.5\n",
      "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
      "Collecting httpcore<0.16.0,>=0.15.0\n",
      "  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rfc3986[idna2008]<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: sniffio in /home/ubuntu/.local/lib/python3.8/site-packages (from httpx->gradio==3.1.4->-r requirements.txt (line 21)) (1.3.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting linkify-it-py~=1.0\n",
      "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
      "Collecting mdit-py-plugins\n",
      "  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib->gradio==3.1.4->-r requirements.txt (line 21)) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib->gradio==3.1.4->-r requirements.txt (line 21)) (4.37.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib->gradio==3.1.4->-r requirements.txt (line 21)) (1.0.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=2.5 in /usr/lib/python3/dist-packages (from paramiko->gradio==3.1.4->-r requirements.txt (line 21)) (2.8)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /usr/lib/python3/dist-packages (from paramiko->gradio==3.1.4->-r requirements.txt (line 21)) (1.3.0)\n",
      "Collecting bcrypt>=3.1.3\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting uc-micro-py\n",
      "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2022.5-py2.py3-none-any.whl (336 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.7/336.7 kB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: albumentations, pudb, fire, antlr4-python3-runtime, test-tube, imgaug, urwid, validators, ffmpy, python-multipart\n",
      "  Building wheel for albumentations (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for albumentations: filename=albumentations-0.4.3-py3-none-any.whl size=60766 sha256=85f4138a7ce35a78d2ad3d9a1c310b789d8063870627b1573289705faf04f79d\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/a0/37/4e/0bd417ba6a58f73329b825623d8c949e8e4ac2cdbd252b786d\n",
      "  Building wheel for pudb (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pudb: filename=pudb-2019.2-py3-none-any.whl size=63230 sha256=513f95514e44a3dcc0d0a46ce33ece4e7ab7a2544151d82132bd6880a190194e\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/48/83/f1/d8a09d401e2512bfda01ac9fc1b334885f9ddf51617c1c49f1\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115925 sha256=ee69078c75c3fd8531e789dea3fd8a659cba3f556584227c7310bebec92d2d88\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/1f/10/06/2a990ee4d73a8479fe2922445e8a876d38cfbfed052284c6a1\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=6251ae644bf7fb6e9f559141626ca67c60d87b4f9882154b9acf9125522f614c\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/c8/d0/ab/d43c02eaddc5b9004db86950802442ad9a26f279c619e28da0\n",
      "  Building wheel for test-tube (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for test-tube: filename=test_tube-0.7.5-py3-none-any.whl size=25358 sha256=c32f1dd16754cd4af814752d9a2b7cecbe207a508023a4c82f7a39f8593f85e7\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/95/b0/3a/00ea66dbb0d9ce470ce1bdcb854a6fa030c279c316cb27ca9e\n",
      "  Building wheel for imgaug (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654018 sha256=061cacbfa961090712f3f03aa4ff171a5bb947b776e3650f288ecdb59c9629df\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/41/23/e8/b1016c275f713978d312621da3c4f55920ec4297798aba8a5a\n",
      "  Building wheel for urwid (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for urwid: filename=urwid-2.1.2-cp38-cp38-linux_x86_64.whl size=259145 sha256=25c956580777249c68783e63a72308fecb8f6439f69904e5ee8fd3171bcfe0ef\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/28/71/e4/38b5d81438105d0e3db5016cf2eea6fa796d89d96a04451d4d\n",
      "  Building wheel for validators (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19565 sha256=b796a10b4edf4618419088704a2a27a0bab1a115e5d1866b43c609407d64f29e\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4709 sha256=a7c5e4d42a79b7e69bf33a97e03b98b5163411e57583e8e84e4e4a12f9a9f30d\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/ff/5b/59/913b443e7369dc04b61f607a746b6f7d83fb65e2e19fcc958d\n",
      "  Building wheel for python-multipart (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31669 sha256=fc4bcf1d1014047ec8cfd12d67448ce4b2c40d841316d4cab09df7c4fa172909\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/9e/fc/1c/cf980e6413d3ee8e70cd8f39e2366b0f487e3e221aeb452eb0\n",
      "Successfully built albumentations pudb fire antlr4-python3-runtime test-tube imgaug urwid validators ffmpy python-multipart\n",
      "Installing collected packages: wcwidth, urwid, tokenizers, rfc3986, pydub, monotonic, ffmpy, einops, commonmark, braceexpand, antlr4-python3-runtime, xxhash, webdataset, watchdog, validators, urllib3, uc-micro-py, tzdata, torch, toml, smmap, semver, rich, regex, python-multipart, pympler, pyDeprecate, pycryptodome, pyarrow, pudb, protobuf, orjson, opencv-python-headless, opencv-python, omegaconf, multidict, mdurl, imageio-ffmpeg, imageio, h11, ftfy, fsspec, frozenlist, fire, dill, bcrypt, backports.zoneinfo, backoff, async-timeout, yarl, uvicorn, torchmetrics, taming-transformers, starlette, pytz-deprecation-shim, pydeck, paramiko, multiprocess, markdown-it-py, linkify-it-py, latent-diffusion, kornia, imgaug, httpcore, gitdb, aiosignal, tzlocal, torchvision, test-tube, responses, mdit-py-plugins, huggingface-hub, httpx, gitpython, fastapi, analytics-python, altair, albumentations, aiohttp, transformers, torch-fidelity, streamlit, diffusers, clip, pytorch-lightning, gradio, datasets\n",
      "  Running setup.py develop for taming-transformers\n",
      "  Running setup.py develop for latent-diffusion\n",
      "  Running setup.py develop for clip\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.2.0 albumentations-0.4.3 altair-4.2.0 analytics-python-1.4.0 antlr4-python3-runtime-4.8 async-timeout-4.0.2 backoff-1.10.0 backports.zoneinfo-0.2.1 bcrypt-4.0.1 braceexpand-0.1.7 clip commonmark-0.9.1 datasets-2.4.0 diffusers-0.3.0 dill-0.3.5.1 einops-0.3.0 fastapi-0.85.1 ffmpy-0.3.0 fire-0.4.0 frozenlist-1.3.1 fsspec-2022.10.0 ftfy-6.1.1 gitdb-4.0.9 gitpython-3.1.29 gradio-3.1.4 h11-0.12.0 httpcore-0.15.0 httpx-0.23.0 huggingface-hub-0.10.1 imageio-2.9.0 imageio-ffmpeg-0.4.2 imgaug-0.2.6 kornia-0.6.0 latent-diffusion-0.0.1 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.1 mdurl-0.1.2 monotonic-1.6 multidict-6.0.2 multiprocess-0.70.13 omegaconf-2.1.1 opencv-python-4.5.5.64 opencv-python-headless-4.6.0.66 orjson-3.8.0 paramiko-2.11.0 protobuf-3.20.3 pudb-2019.2 pyDeprecate-0.3.1 pyarrow-9.0.0 pycryptodome-3.15.0 pydeck-0.8.0b4 pydub-0.25.1 pympler-1.0.1 python-multipart-0.0.5 pytorch-lightning-1.4.2 pytz-deprecation-shim-0.1.0.post0 regex-2022.9.13 responses-0.18.0 rfc3986-1.5.0 rich-12.6.0 semver-2.13.0 smmap-5.0.0 starlette-0.20.4 streamlit-1.13.0 taming-transformers-0.0.1 test-tube-0.7.5 tokenizers-0.12.1 toml-0.10.2 torch-1.12.1+cu113 torch-fidelity-0.3.0 torchmetrics-0.6.0 torchvision-0.13.1+cu113 transformers-4.22.2 tzdata-2022.5 tzlocal-4.2 uc-micro-py-1.0.1 urllib3-1.26.12 urwid-2.1.2 uvicorn-0.19.0 validators-0.20.0 watchdog-2.1.9 wcwidth-0.2.5 webdataset-0.2.5 xxhash-3.1.0 yarl-1.8.1\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/home/ubuntu/.local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1458, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='22.3'),)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /usr/lib/python3/dist-packages (2.9.0)\n",
      "Collecting keras\n",
      "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras\n",
      "Successfully installed keras-2.10.0\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/home/ubuntu/.local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1458, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='22.3'),)\n",
      "\u001b[33mWARNING: Skipping torchtext as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in /home/ubuntu/.local/lib/python3.8/site-packages (0.10.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface_hub) (4.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub) (5.3.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface_hub) (2.28.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface_hub) (3.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.9->huggingface_hub) (2.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->huggingface_hub) (1.26.12)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/home/ubuntu/.local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1458, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='22.3'),)\n"
     ]
    }
   ],
   "source": [
    "# Note: Run this cell, then you'll need to click the \"Restart\" button that pops up\n",
    "# this is a result of the installation, and I don't think there's any way around this.\n",
    "\n",
    "#!git clone https://github.com/devonbrackbill/stable-diffusion.git\n",
    "#%cd stable-diffusion\n",
    "#!pip install --upgrade pip\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "!pip install --upgrade keras # on lambda stack we need to upgrade keras\n",
    "!pip uninstall -y torchtext # on colab we need to remove torchtext\n",
    "\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wM1Ip5p13WEv"
   },
   "source": [
    "# 1. Download Image Icons from FontAwesome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUtYxXRKydSi",
    "outputId": "718ca528-f3fe-44ce-c831-c897f6a36b80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting cairosvg\n",
      "  Downloading CairoSVG-2.5.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.8/site-packages (1.4.4)\n",
      "Requirement already satisfied: tinycss2 in /home/ubuntu/.local/lib/python3.8/site-packages (from cairosvg) (1.1.1)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from cairosvg) (7.0.0)\n",
      "Collecting cssselect2\n",
      "  Downloading cssselect2-0.7.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: defusedxml in /usr/lib/python3/dist-packages (from cairosvg) (0.6.0)\n",
      "Collecting cairocffi\n",
      "  Downloading cairocffi-1.4.0.tar.gz (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas) (1.23.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "Requirement already satisfied: cffi>=1.1.0 in /usr/lib/python3/dist-packages (from cairocffi->cairosvg) (1.14.0)\n",
      "Requirement already satisfied: webencodings in /usr/lib/python3/dist-packages (from cssselect2->cairosvg) (0.5.1)\n",
      "Building wheels for collected packages: cairocffi\n",
      "  Building wheel for cairocffi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cairocffi: filename=cairocffi-1.4.0-py3-none-any.whl size=88776 sha256=64a811cb69ea0e42c36cae12657950203e47d8c95726a5af0b56b32f9515febf\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/01/a9/c0/5c05f9dd73c21f9a7716690642823cdba55594d17a9bd69daf\n",
      "Successfully built cairocffi\n",
      "Installing collected packages: cairocffi, cssselect2, cairosvg\n",
      "Successfully installed cairocffi-1.4.0 cairosvg-2.5.2 cssselect2-0.7.0\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1673, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1305, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/segment.py\", line 249, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/console.py\", line 1283, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/home/ubuntu/.local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/cli/req_command.py\", line 190, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/self_outdated_check.py\", line 236, in pip_self_version_check\n",
      "    logger.warning(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1458, in warning\n",
      "    self._log(WARNING, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.2.2', new='22.3'),)\n"
     ]
    }
   ],
   "source": [
    "!pip install cairosvg pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qUcSZPsaymh9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "import cairosvg\n",
    "import pandas as pd\n",
    "import json\n",
    "import cv2\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LUuGfLFQ4Uee",
    "outputId": "3b8caa6f-108d-496f-9b9f-dbfa93d566e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created /res directory\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.mkdir('res')\n",
    "    print('created /res directory')\n",
    "except OSError:\n",
    "    print(\"res/ already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5XdFUwxs4Wlr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-10-25 20:49:08--  https://use.fontawesome.com/releases/v6.2.0/fontawesome-free-6.2.0-web.zip\n",
      "Resolving use.fontawesome.com (use.fontawesome.com)... 2606:4700:e2::ac40:850f, 2606:4700:e2::ac40:840f, 172.64.133.15, ...\n",
      "Connecting to use.fontawesome.com (use.fontawesome.com)|2606:4700:e2::ac40:850f|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6370953 (6.1M) [application/zip]\n",
      "Saving to: ‘res/fontawesome-free-6.2.0-web.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 1.46M 4s\n",
      "    50K .......... .......... .......... .......... ..........  1% 2.67M 3s\n",
      "   100K .......... .......... .......... .......... ..........  2% 2.09M 3s\n",
      "   150K .......... .......... .......... .......... ..........  3% 6.56M 2s\n",
      "   200K .......... .......... .......... .......... ..........  4% 5.81M 2s\n",
      "   250K .......... .......... .......... .......... ..........  4% 3.23M 2s\n",
      "   300K .......... .......... .......... .......... ..........  5% 6.41M 2s\n",
      "   350K .......... .......... .......... .......... ..........  6% 4.74M 2s\n",
      "   400K .......... .......... .......... .......... ..........  7% 9.81M 2s\n",
      "   450K .......... .......... .......... .......... ..........  8% 8.43M 2s\n",
      "   500K .......... .......... .......... .......... ..........  8% 5.08M 1s\n",
      "   550K .......... .......... .......... .......... ..........  9% 8.38M 1s\n",
      "   600K .......... .......... .......... .......... .......... 10% 17.2M 1s\n",
      "   650K .......... .......... .......... .......... .......... 11% 10.3M 1s\n",
      "   700K .......... .......... .......... .......... .......... 12% 21.1M 1s\n",
      "   750K .......... .......... .......... .......... .......... 12% 6.22M 1s\n",
      "   800K .......... .......... .......... .......... .......... 13%  319M 1s\n",
      "   850K .......... .......... .......... .......... .......... 14% 28.9M 1s\n",
      "   900K .......... .......... .......... .......... .......... 15% 6.20M 1s\n",
      "   950K .......... .......... .......... .......... .......... 16%  281M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 16% 15.5M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 17% 19.8M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 18% 15.5M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 19% 12.6M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 20% 15.2M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 20% 24.1M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 21% 21.0M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 22% 21.2M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 23% 22.0M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 24% 24.6M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 24% 21.2M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 25% 13.9M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 26% 35.5M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 27% 12.2M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 28%  301M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 28% 15.9M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 29% 19.8M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 30% 32.7M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 31% 62.0M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 32% 15.1M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 32% 22.1M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 33% 27.0M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 34% 25.2M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 35% 26.2M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 36% 91.1M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 36% 26.7M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 37% 70.2M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 38% 13.3M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 39% 28.7M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 40% 25.8M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 40%  178M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 41% 27.4M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 42% 31.0M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 43% 49.2M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 44% 27.8M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 45% 28.7M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 45% 24.5M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 46% 48.9M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 47% 17.2M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 48% 34.4M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 49%  108M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 49% 18.7M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 50% 54.1M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 51% 8.62M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 52%  140M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 53%  373M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 53% 27.6M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 54% 34.7M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 55% 34.1M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 56% 31.3M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 57% 24.7M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 57% 19.5M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 58% 64.2M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 59% 30.0M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 60% 12.9M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 61% 43.4M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 61% 56.4M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 62% 61.6M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 63% 35.9M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 64% 28.2M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 65% 23.0M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 65% 27.9M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 66% 22.8M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 67% 50.8M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 68% 21.4M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 69%  133M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 69% 41.9M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 70% 13.2M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 71% 43.2M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 72% 24.5M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 73% 29.9M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 73% 30.4M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 74% 27.5M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 75% 26.2M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 76% 33.0M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 77% 14.7M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 77% 23.1M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 78% 33.4M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 79% 24.6M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 80% 30.5M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 81% 16.4M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 81%  339M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 82% 21.7M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 83% 35.3M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 84% 22.1M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 85% 36.3M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 85% 17.6M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 86% 20.0M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 87% 31.1M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 88% 63.6M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 89% 28.8M 0s\n",
      "  5550K .......... .......... .......... .......... .......... 90% 10.1M 0s\n",
      "  5600K .......... .......... .......... .......... .......... 90% 57.4M 0s\n",
      "  5650K .......... .......... .......... .......... .......... 91% 67.9M 0s\n",
      "  5700K .......... .......... .......... .......... .......... 92% 30.4M 0s\n",
      "  5750K .......... .......... .......... .......... .......... 93% 23.8M 0s\n",
      "  5800K .......... .......... .......... .......... .......... 94%  376M 0s\n",
      "  5850K .......... .......... .......... .......... .......... 94% 31.3M 0s\n",
      "  5900K .......... .......... .......... .......... .......... 95% 43.0M 0s\n",
      "  5950K .......... .......... .......... .......... .......... 96% 17.9M 0s\n",
      "  6000K .......... .......... .......... .......... .......... 97% 28.1M 0s\n",
      "  6050K .......... .......... .......... .......... .......... 98% 11.5M 0s\n",
      "  6100K .......... .......... .......... .......... .......... 98% 45.3M 0s\n",
      "  6150K .......... .......... .......... .......... .......... 99% 52.7M 0s\n",
      "  6200K .......... .......... .                               100%  240M=0.4s\n",
      "\n",
      "2022-10-25 20:49:08 (16.3 MB/s) - ‘res/fontawesome-free-6.2.0-web.zip’ saved [6370953/6370953]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fetch Fontawesome\n",
    "if len([file for file in os.listdir('res/') if 'fontawesome' in file]) == 0:\n",
    "    subprocess.call(\n",
    "        [\"wget\", \n",
    "        \"https://use.fontawesome.com/releases/v6.2.0/fontawesome-free-6.2.0-web.zip\", \n",
    "        \"--directory-prefix=res\"])\n",
    "else:\n",
    "    print(\"fontawesome is already downloaded: {}\".format([file for file in os.listdir('res/') if 'fontawesome' in file]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pq2u2zp34bPg",
    "outputId": "f5009af1-16a9-43f2-b05d-df5b5020c225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FontAwesome files saved in: res/fontawesome-free-6.2.0-web\n"
     ]
    }
   ],
   "source": [
    "# unzip\n",
    "fontawesome_dir = ['res/' + file for file in os.listdir('res/') if 'fontawesome' in file and '.zip' in file][0]\n",
    "\n",
    "with zipfile.ZipFile(fontawesome_dir,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"res\")\n",
    "\n",
    "fontawesome_dir = ['res/' + file for file in os.listdir('res/') if 'fontawesome' in file and 'zip' not in file][0]\n",
    "\n",
    "print('FontAwesome files saved in: {}'.format(fontawesome_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jaeAZjL64cYc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image does not have 4 channels; deleting it: res/fontawesome-png/square-full.png\n"
     ]
    }
   ],
   "source": [
    "# read svg file -> png data\n",
    "WIDTH = 512\n",
    "HEIGHT = 512\n",
    "png_dir = 'res/fontawesome-png'\n",
    "\n",
    "try:\n",
    "    os.mkdir(png_dir)\n",
    "except OSError:\n",
    "    print(\"{} already exists\".format(png_dir))\n",
    "\n",
    "files = sorted(os.listdir(os.path.join(fontawesome_dir, 'svgs', 'solid')))\n",
    "\n",
    "filenames = [file.split('.svg')[0] for file in files]\n",
    "textdescrip = [file.replace('-', ' ') for file in filenames]\n",
    "filenames_png = [file + '.png' for file in filenames]\n",
    "\n",
    "for file, filename in zip(files, filenames):\n",
    "    cairosvg.svg2png(url=os.path.join(fontawesome_dir, 'svgs', 'solid', file), \n",
    "                     output_width=WIDTH, \n",
    "                     output_height=HEIGHT,\n",
    "                     write_to=os.path.join(png_dir, filename + '.png'))\n",
    "    # read in the image\n",
    "    img = cv2.imread(os.path.join(png_dir, filename + '.png'), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "    if len(img.shape) and img.shape[2] == 4:\n",
    "\n",
    "        # change black -> white and white -> black\n",
    "        img[:,:,0] = 255-img[:,:,3]\n",
    "        img[:,:,1] = 255-img[:,:,3]\n",
    "        img[:,:,2] = 255-img[:,:,3]\n",
    "        # remove 4th color channel (4th is alpha channel)\n",
    "        img = img[:, :, :3]\n",
    "\n",
    "        cv2.imwrite(os.path.join(png_dir, filename + '.png'), img)\n",
    "    else:\n",
    "        print(\"Image does not have 4 channels; deleting it: {}\".format(os.path.join(png_dir, filename + '.png')))\n",
    "        os.unlink(os.path.join(png_dir, filename + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "swMtoHEx4c-z",
    "outputId": "e73aff98-d378-4944-cf10-3c6dd66ad924"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>xmarks-lines.png</td>\n",
       "      <td>xmarks lines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>y.png</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>yen-sign.png</td>\n",
       "      <td>yen sign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>yin-yang.png</td>\n",
       "      <td>yin yang</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>z.png</td>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file_name          text\n",
       "1383  xmarks-lines.png  xmarks lines\n",
       "1384             y.png             y\n",
       "1385      yen-sign.png      yen sign\n",
       "1386      yin-yang.png      yin yang\n",
       "1387             z.png             z"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames_df = pd.DataFrame({#'file_name' : files,\n",
    "                             'file_name': filenames_png,\n",
    "                             'text': textdescrip})\n",
    "filenames_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Xov59TJk_h9v"
   },
   "outputs": [],
   "source": [
    "metadata = []\n",
    "for idx, row in filenames_df.iterrows():\n",
    "    metadata.append({'file_name': row['file_name'],\n",
    "                   'text': row['text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PHWhUAZ7CqhD"
   },
   "outputs": [],
   "source": [
    "metadata_list_of_strs = []\n",
    "for item in metadata:\n",
    "    metadata_list_of_strs.append(json.dumps(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DRY0V8zoC0My"
   },
   "outputs": [],
   "source": [
    "# metadata_string = '\\n'.join(metadata_list_of_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "i8Kz4KfIEK-s"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(png_dir, 'metadata.txt'), 'w') as f:\n",
    "  # f.writelines(metadata_list_of_strs)\n",
    "  for line in metadata_list_of_strs:\n",
    "      f.write(line + '\\n')\n",
    "\n",
    "# convert to .jsonl file\n",
    "os.rename(os.path.join(png_dir, 'metadata.txt'), os.path.join(png_dir, 'metadata.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "jtOy6LVGD5kG",
    "outputId": "bd900f34-13a9-4c60-d233-4d9b60c3143a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>train-subway.png</td>\n",
       "      <td>train subway</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>train-tram.png</td>\n",
       "      <td>train tram</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>train.png</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             file_name          text\n",
       "1246  train-subway.png  train subway\n",
       "1247    train-tram.png    train tram\n",
       "1248         train.png         train"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames_df[filenames_df['file_name'].str.contains('train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Chbqh56pCDCy",
    "outputId": "9032a0d6-d2f5-4176-ce96-9ffdb2fdbed6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['metadata.jsonl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[file for file in os.listdir(png_dir) if 'json' in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYUI6v8P5a1s"
   },
   "source": [
    "# 2. Convert to Right format for Stable Diffusion Pytorch (HuggingFace DataSet object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f4WhboH25vor"
   },
   "outputs": [],
   "source": [
    "# Check the dataset\n",
    "from datasets import load_dataset\n",
    "# ds = load_dataset(\"lambdalabs/pokemon-blip-captions\", split=\"train\")\n",
    "# sample = ds[0]\n",
    "# display(sample[\"image\"].resize((256, 256)))\n",
    "# print(sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "6ef4cc3bdc414253876b7c192c317821",
      "1ae6b0e7e281462da09c94cc374db817",
      "9f534add57ec46c892188e50493f68a4",
      "dc89c8da284f434a9fb342e0ae75974c",
      "c34caa8a4d854f3795da9c744bddecce",
      "c024e14d3599420d93161add7bbd1710",
      "20c9a7ec40174f99a2031d24f3669b7e",
      "300cc2fb8b7c4205be1cab3932819161",
      "2bb60c219d8a4d28b5278d5beb0f6bd8",
      "2097917a39ae406e9e9efaeca325ee6c",
      "c210d9364b05491a81fe59ef47b2de22",
      "f65c0c8fcb6d4801b30498bf870480de",
      "ab3b91591fcb43dcb4f17c9dde29e703",
      "64bae32df01b4991998f93ae8e67b757",
      "1f1fbb662039413ca4a9d2117c675f03",
      "5f5d1cd4b2e94d0b9f12de0c690f8eb0",
      "8b43934684f24e00b4c829309b4de6a5",
      "c6d1b7331c484f66b5cb123c14630493",
      "d9618475279940c181240d507a2d375d",
      "9021a311595f4e2988218a0058730c6b",
      "0a2719d9a1c543ffa15182db8b0200c1",
      "286d9cabc07141c28061f35915526e92",
      "9480529489394add835edbd5fb8f69fa",
      "772aedd2014c437087170ddfb83b9b19",
      "010b4e607e1b47299ce1acb046626c31",
      "ae94f7cb3e584375988baea3f1f4a9b5",
      "8b11c88266db4ba88b87903f2e4c7330",
      "02507c3b7dfa409ab1ba39fffd36b426",
      "778966d743d74f0a90cb6ae480683919",
      "fbddeda33caf456d874de65a8f4bd1f0",
      "6e7cb796b6d8411d9d7f0e0a833ab00c",
      "281a6fc232024541ba2bc170ab98391f",
      "ef5892d7a3d34745925966716e07f17f",
      "ac7fa3a7181444cfbe357cde9cbaec91",
      "010488febefd4d529784ae4311c9a31c",
      "7394cd1240824a9096158d9aef264174",
      "87a99ca1db6443acb772006d8bb44743",
      "0d4eade367284cdd9d113638c9ad474c",
      "3b6559b90a93416c8511369561a194db",
      "f48747d959274a659dcd46a0e6dd0a3d",
      "a264bec25d0641ceb22dcc4052198e87",
      "3f25f5eb265042ab88138831af7df61e",
      "765096396ecf47c6aceb46f12556cee9",
      "bdb417b6db524af29a600f64fc387df1"
     ]
    },
    "id": "XVvE3ryH6cch",
    "outputId": "f72c2b8d-28aa-4da6-d3e8-889bcd986097"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e9acb79c66f0c46a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset imagefolder/default to /home/ubuntu/.cache/huggingface/datasets/imagefolder/default-e9acb79c66f0c46a/0.0.0/0fc50c79b681877cc46b23245a6ef5333d036f48db40d53765a68034bc48faff...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efc4d2e45a44a3a93c4479e92d90cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786cf243d75e4bd182e75ee44e33def0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fe01adbdf441829da2d269a044adf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imagefolder downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/imagefolder/default-e9acb79c66f0c46a/0.0.0/0fc50c79b681877cc46b23245a6ef5333d036f48db40d53765a68034bc48faff. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# ds = load_dataset(\"imagefolder\", \n",
    "#                   data_files={\"train\": \"res/fontawesome-png/**\", \n",
    "#                               #\"test\": \"path/to/test/**\", \n",
    "#                               #\"valid\": \"path/to/valid/**\"\n",
    "#                               })\n",
    "ds = load_dataset(\"imagefolder\", \n",
    "                  data_dir=\"res/fontawesome-png/\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "id": "S22Dazn47aok",
    "outputId": "8ae25bf6-f46c-4ab7-887b-5824b7a901b9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=512x512 at 0x7F0249E3E130>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train subway\n"
     ]
    }
   ],
   "source": [
    "sample = ds[0]\n",
    "display(sample[\"image\"])\n",
    "print(sample[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsTPHMqj8LZ0",
    "outputId": "013171d1-95d3-49f2-8ce5-d16b5fab9ea9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=512x512 at 0x7F0249E3E130>,\n",
       " 'text': 'train subway'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDXWCCVx5p4K"
   },
   "source": [
    "# 3. Fine Tune Stable Diffusion\n",
    "\n",
    "copied from: https://github.com/LambdaLabsML/examples/blob/main/stable-diffusion-finetuning/pokemon_finetune.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELHtL46m80Z2",
    "outputId": "fe5a5c74-a8e8-47af-8179-0d7b6baf5cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 25 11:30:31 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    42W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ycuj14Qx6M-_"
   },
   "outputs": [],
   "source": [
    "# from google.colab import output\n",
    "# output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642,
     "referenced_widgets": [
      "537aefd8082140468cca39281d09bf8a",
      "06c00c02914e42459262d60a7dd73a34",
      "016a0ac954e14f8cb08dc697a9d9f4bc",
      "7156868013504219a4cb908f2d558d13",
      "06c1828f3c31435daee428bcc805c799",
      "ea9ed7901911487eaa3ea1a61020727a",
      "d9e8a54cc0f34a3ba4e918eecdbb41af",
      "ea655bd18fa14b479f2a4981b8c6b930",
      "9df0a9bebfe24df08b82a87bc89cfb8a",
      "3596851805c142678d34ad3c6dd9e762",
      "1539d9c737b54ebda13a2b7b118fce91",
      "02708c54eb0e4dd8a32634b720c49a2d",
      "cbc46aa7f74f491985f4e5abb924fd28",
      "a7768c97734d4e49a97d18f3c20b8a34"
     ]
    },
    "id": "AJ_b9AYa5uUv",
    "outputId": "807466b0-243c-414d-f863-6c8f19716876"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519c51e97ccb4547b87785d17242ea45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "aaa3a1e8efb443969baf60df9cb1aff8",
      "464a8a73387a4b33a09ea4ac1c451981",
      "02e7dc6161f54f8eaed207f2070e5229",
      "04a14256e3154facbe6f39bccd161559",
      "6f7a17bd622842e1b7bcf9f7188abe05",
      "20432153e5504425a1c3fa8b1d710022",
      "a2da03b0e3404d4e91a43a28c2958b1e",
      "99ad6c06e68b486cbc5f89ed47cc0669",
      "632776253370416da2473354070f017a",
      "0cd1f5e4df034a62abb04ccdfc0ac737",
      "ba85474177f84bc18bc6cafa700e1199"
     ]
    },
    "id": "BAFLtNan6Ben",
    "outputId": "fc776905-16f0-4371-9057-ddebcd5aff53"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177d93cf0c4e4b30aa5e22af004e06d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/7.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "ckpt_path = hf_hub_download(repo_id=\"CompVis/stable-diffusion-v-1-4-original\", \n",
    "                            filename=\"sd-v1-4-full-ema.ckpt\", \n",
    "                            use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qHa2JQ37JYC",
    "outputId": "c9bd7b21-43b7-429d-a75b-d0c63edde2e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPUs: 0\n"
     ]
    }
   ],
   "source": [
    "# 2xA6000:\n",
    "\n",
    "# BATCH_SIZE = 4\n",
    "BATCH_SIZE = 1\n",
    "N_GPUS = 1\n",
    "ACCUMULATE_BATCHES = 1\n",
    "\n",
    "if N_GPUS > 1:\n",
    "  gpu_list = \",\".join((str(x) for x in range(N_GPUS))) + \",\"\n",
    "else:\n",
    "  gpu_list = \"0\"\n",
    "print(f\"Using GPUs: {gpu_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0jB9QWNaNDdO",
    "outputId": "abf3d82a-549c-4ad4-89e5-01bb6f0609fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/stable-diffusion\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "3M7-5f8uBnkf",
    "outputId": "b4925a24-6e5f-4619-af53-aafa400552dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/.cache/huggingface/hub/models--CompVis--stable-diffusion-v-1-4-original/snapshots/0834a76f88354683d3f7ef271cadd28f4757a8cc/sd-v1-4-full-ema.ckpt'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qgDjM4ZQGoEY",
    "outputId": "2353d8ab-51ea-4ecf-89df-9e21ebd90e03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v-1-4-original/snapshots/0834a76f88354683d3f7ef271cadd28f4757a8cc/sd-v1-4-full-ema.ckpt': Permission denied\n"
     ]
    }
   ],
   "source": [
    "! ls /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v-1-4-original/snapshots/0834a76f88354683d3f7ef271cadd28f4757a8cc/sd-v1-4-full-ema.ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdNc-eDo-gAl"
   },
   "source": [
    "Note:\n",
    "\n",
    "Here, I needed to add a new helper function in `stable-diffusion/ldm/data/simple` called `hf_dataset_from_local()` with the following code:\n",
    "\n",
    "```\n",
    "def hf_dataset_from_local(\n",
    "    name,\n",
    "    data_dir,\n",
    "    image_transforms=[],\n",
    "    image_column=\"image\",\n",
    "    text_column=\"text\",\n",
    "    split='train',\n",
    "    image_key='image',\n",
    "    caption_key='txt',\n",
    "    ):\n",
    "    \"\"\"Make huggingface dataset with appropriate list of transforms applied\n",
    "    \"\"\"\n",
    "    ds = load_dataset(name, data_dir=data_dir, split=split)\n",
    "    image_transforms = [instantiate_from_config(tt) for tt in image_transforms]\n",
    "    image_transforms.extend([transforms.ToTensor(),\n",
    "                                transforms.Lambda(lambda x: rearrange(x * 2. - 1., 'c h w -> h w c'))])\n",
    "    tform = transforms.Compose(image_transforms)\n",
    "\n",
    "    assert image_column in ds.column_names, f\"Didn't find column {image_column} in {ds.column_names}\"\n",
    "    assert text_column in ds.column_names, f\"Didn't find column {text_column} in {ds.column_names}\"\n",
    "\n",
    "    def pre_process(examples):\n",
    "        processed = {}\n",
    "        processed[image_key] = [tform(im) for im in examples[image_column]]\n",
    "        processed[caption_key] = examples[text_column]\n",
    "        return processed\n",
    "\n",
    "    ds.set_transform(pre_process)\n",
    "    return ds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "IGmua1ng8TB1"
   },
   "outputs": [],
   "source": [
    "yaml = \\\n",
    "\"\"\"model:\n",
    "  base_learning_rate: 1.0e-04\n",
    "  target: ldm.models.diffusion.ddpm.LatentDiffusion\n",
    "  params:\n",
    "    linear_start: 0.00085\n",
    "    linear_end: 0.0120\n",
    "    num_timesteps_cond: 1\n",
    "    log_every_t: 200\n",
    "    timesteps: 1000\n",
    "    first_stage_key: \"image\"\n",
    "    cond_stage_key: \"txt\"\n",
    "    image_size: 64\n",
    "    channels: 4\n",
    "    cond_stage_trainable: false   # Note: different from the one we trained before\n",
    "    conditioning_key: crossattn\n",
    "    scale_factor: 0.18215\n",
    "\n",
    "    scheduler_config: # 10000 warmup steps\n",
    "      target: ldm.lr_scheduler.LambdaLinearScheduler\n",
    "      params:\n",
    "        warm_up_steps: [ 1 ] # NOTE for resuming. use 10000 if starting from scratch\n",
    "        cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases\n",
    "        f_start: [ 1.e-6 ]\n",
    "        f_max: [ 1. ]\n",
    "        f_min: [ 1. ]\n",
    "\n",
    "    unet_config:\n",
    "      target: ldm.modules.diffusionmodules.openaimodel.UNetModel\n",
    "      params:\n",
    "        image_size: 32 # unused\n",
    "        in_channels: 4\n",
    "        out_channels: 4\n",
    "        model_channels: 320\n",
    "        attention_resolutions: [ 4, 2, 1 ]\n",
    "        num_res_blocks: 2\n",
    "        channel_mult: [ 1, 2, 4, 4 ]\n",
    "        num_heads: 8\n",
    "        use_spatial_transformer: True\n",
    "        transformer_depth: 1\n",
    "        context_dim: 768\n",
    "        use_checkpoint: True\n",
    "        legacy: False\n",
    "\n",
    "    first_stage_config:\n",
    "      target: ldm.models.autoencoder.AutoencoderKL\n",
    "      ckpt_path: \"models/first_stage_models/kl-f8/model.ckpt\"\n",
    "      params:\n",
    "        embed_dim: 4\n",
    "        monitor: val/rec_loss\n",
    "        ddconfig:\n",
    "          double_z: true\n",
    "          z_channels: 4\n",
    "          resolution: 256\n",
    "          in_channels: 3\n",
    "          out_ch: 3\n",
    "          ch: 128\n",
    "          ch_mult:\n",
    "          - 1\n",
    "          - 2\n",
    "          - 4\n",
    "          - 4\n",
    "          num_res_blocks: 2\n",
    "          attn_resolutions: []\n",
    "          dropout: 0.0\n",
    "        lossconfig:\n",
    "          target: torch.nn.Identity\n",
    "\n",
    "    cond_stage_config:\n",
    "      target: ldm.modules.encoders.modules.FrozenCLIPEmbedder\n",
    "\n",
    "\n",
    "data:\n",
    "  target: main.DataModuleFromConfig\n",
    "  params:\n",
    "    batch_size: 4\n",
    "    num_workers: 4\n",
    "    num_val_workers: 0 # Avoid a weird val dataloader issue\n",
    "    train:\n",
    "      target: ldm.data.simple.hf_dataset_from_local\n",
    "      params:\n",
    "        name: imagefolder\n",
    "        data_dir: res/fontawesome-png\n",
    "        image_transforms:\n",
    "        - target: torchvision.transforms.Resize\n",
    "          params:\n",
    "            size: 512\n",
    "            interpolation: 3\n",
    "        - target: torchvision.transforms.RandomCrop\n",
    "          params:\n",
    "            size: 512\n",
    "        # - target: torchvision.transforms.RandomHorizontalFlip\n",
    "        text_column: text\n",
    "        image_column: image\n",
    "        caption_key: text\n",
    "    validation:\n",
    "      target: ldm.data.simple.TextOnly\n",
    "      params:\n",
    "        captions:\n",
    "        - \"radar\"\n",
    "        - \"bunny rabbit\"\n",
    "        - \"Yoda\"\n",
    "        - \"coffee\"\n",
    "        - \"palm tree\"\n",
    "        - \"evergreen tree\"\n",
    "        - \"toucan\"\n",
    "        - \"thumbs up\"\n",
    "        - \"butterfly\"\n",
    "        output_size: 512\n",
    "        n_gpus: 1 # small hack to sure we see all our samples; # I changed this to 1\n",
    "\n",
    "\n",
    "lightning:\n",
    "  find_unused_parameters: False\n",
    "\n",
    "  modelcheckpoint:\n",
    "    params:\n",
    "      every_n_train_steps: 2000\n",
    "      save_top_k: -1\n",
    "      monitor: null\n",
    "\n",
    "  callbacks:\n",
    "    image_logger:\n",
    "      target: main.ImageLogger\n",
    "      params:\n",
    "        batch_frequency: 2000\n",
    "        max_images: 4\n",
    "        increase_log_steps: False\n",
    "        log_first_step: True\n",
    "        log_all_val: True\n",
    "        log_images_kwargs:\n",
    "          use_ema_scope: True\n",
    "          inpaint: False\n",
    "          plot_progressive_rows: False\n",
    "          plot_diffusion_rows: False\n",
    "          N: 4\n",
    "          unconditional_guidance_scale: 3.0\n",
    "          unconditional_guidance_label: [\"\"]\n",
    "\n",
    "  trainer:\n",
    "    benchmark: True\n",
    "    num_sanity_val_steps: 0\n",
    "    accumulate_grad_batches: 1\n",
    "\"\"\"\n",
    "\n",
    "with open('configs/stable-diffusion/retrain-icons.yaml', 'w') as f:\n",
    "  f.write(yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "F-HSPjK0CBI5",
    "outputId": "e27e8135-d2d4-4299-d1c7-e2b0d8005e86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hu8vaz8jFIaN",
    "outputId": "edeeccc0-eb5a-412b-8fda-4f0c91915a2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE\t\t\t    im-examples\t\t       requirements.txt\n",
      "README.md\t\t    latent_diffusion.egg-info  res\n",
      "StableDiffusionIcons.ipynb  ldm\t\t\t       scripts\n",
      "assets\t\t\t    main.py\t\t       setup.py\n",
      "configs\t\t\t    models\t\t       src\n",
      "data\t\t\t    notebook_helpers.py\n",
      "examples\t\t    notebooks\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "dtKiDJ8yCnpU"
   },
   "outputs": [],
   "source": [
    "# # Run training (original command)\n",
    "# !(python main.py \\\n",
    "#     -t \\\n",
    "#     --base configs/stable-diffusion/retrain-icons.yaml \\\n",
    "#     --gpus \"$gpu_list\" \\\n",
    "#     --scale_lr False \\\n",
    "#     --num_nodes 1 \\\n",
    "#     --check_val_every_n_epoch 10 \\\n",
    "#     --finetune_from \"$ckpt_path\" \\\n",
    "#     data.params.batch_size=\"$BATCH_SIZE\" \\\n",
    "#     lightning.trainer.accumulate_grad_batches=\"$ACCUMULATE_BATCHES\" \\\n",
    "#     data.params.validation.params.n_gpus=\"$NUM_GPUS\" \\\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pillow:  RGBA (512, 512)\n"
     ]
    }
   ],
   "source": [
    "# img_pil = Image.open('res/fontawesome-png/anchor-lock.png')\n",
    "# print('Pillow: ', img_pil.mode, img_pil.size)\n",
    "\n",
    "# img = cv2.imread('res/fontawesome-png/anchor-lock.png', cv2.IMREAD_UNCHANGED)\n",
    "# print('OpenCV: ', img.shape)\n",
    "\n",
    "# img = cv2.imread('res/fontawesome-png/anchor-lock.png', cv2.IMREAD_UNCHANGED)\n",
    "# print('OpenCV: ', img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rLL8j-MFpayG",
    "outputId": "bc5aae21-54ed-43ba-b9b4-8f61141e5c3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "Moving 0 files to the new cache system\n",
      "0it [00:00, ?it/s]\n",
      "Global seed set to 23\n",
      "Running on GPUs 1\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "Keeping EMAs of 688.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Downloading: \"https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth\" to /home/ubuntu/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth\n",
      "100%|███████████████████████████████████████| 5.10M/5.10M [00:00<00:00, 289MB/s]\n",
      "Downloading: 100%|███████████████████████████| 961k/961k [00:00<00:00, 23.7MB/s]\n",
      "Downloading: 100%|███████████████████████████| 525k/525k [00:00<00:00, 25.4MB/s]\n",
      "Downloading: 100%|██████████████████████████████| 389/389 [00:00<00:00, 262kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 905/905 [00:00<00:00, 953kB/s]\n",
      "Downloading: 100%|█████████████████████████| 4.52k/4.52k [00:00<00:00, 2.63MB/s]\n",
      "Downloading: 100%|█████████████████████████| 1.71G/1.71G [00:25<00:00, 66.8MB/s]\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'logit_scale', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'visual_projection.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'text_projection.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.post_layernorm.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Attempting to load state from /home/ubuntu/.cache/huggingface/hub/models--CompVis--stable-diffusion-v-1-4-original/snapshots/0834a76f88354683d3f7ef271cadd28f4757a8cc/sd-v1-4-full-ema.ckpt\n",
      "Found nested key 'state_dict' in checkpoint, loading this instead\n",
      "Monitoring val/loss as checkpoint metric.\n",
      "Merged modelckpt-cfg: \n",
      "{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2022-10-25T11-33-06_retrain-icons/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': None, 'save_top_k': -1, 'every_n_train_steps': 2000}}\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:432: UserWarning: ModelCheckpoint(save_last=True, save_top_k=None, monitor=None) is a redundant configuration. You can save the last checkpoint with ModelCheckpoint(save_top_k=None, monitor=None).\n",
      "  rank_zero_warn(\n",
      "ModelCheckpoint(save_last=True, save_top_k=-1, monitor=None) will duplicate the last checkpoint saved.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Using custom data configuration default-ca61c26b7e011f08\n",
      "Downloading and preparing dataset imagefolder/default to /home/ubuntu/.cache/huggingface/datasets/imagefolder/default-ca61c26b7e011f08/0.0.0/0fc50c79b681877cc46b23245a6ef5333d036f48db40d53765a68034bc48faff...\n",
      "Downloading data files: 100%|██████████████████| 4/4 [00:00<00:00, 32513.98it/s]\n",
      "Downloading data files: 0it [00:00, ?it/s]\n",
      "Extracting data files: 0it [00:00, ?it/s]\n",
      "Dataset imagefolder downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/imagefolder/default-ca61c26b7e011f08/0.0.0/0fc50c79b681877cc46b23245a6ef5333d036f48db40d53765a68034bc48faff. Subsequent calls will reuse this data.\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "Using custom data configuration default-ca61c26b7e011f08\n",
      "Reusing dataset imagefolder (/home/ubuntu/.cache/huggingface/datasets/imagefolder/default-ca61c26b7e011f08/0.0.0/0fc50c79b681877cc46b23245a6ef5333d036f48db40d53765a68034bc48faff)\n",
      "#### Data #####\n",
      "train, Dataset, 3\n",
      "validation, TextOnly, 9\n",
      "accumulate_grad_batches = 1\n",
      "++++ NOT USING LR SCALING ++++\n",
      "Setting learning rate to 1.00e-04\n",
      "Global seed set to 23\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Training the full unet\n",
      "Setting up LambdaLR scheduler...\n",
      "--------------------------------------------------------------------------\n",
      "WARNING: No preset parameters were found for the device that Open MPI\n",
      "detected:\n",
      "\n",
      "  Local host:            150-136-218-69\n",
      "  Device name:           mlx5_0\n",
      "  Device vendor ID:      0x02c9\n",
      "  Device vendor part ID: 4126\n",
      "\n",
      "Default device parameters will be used, which may result in lower\n",
      "performance.  You can edit any of the files specified by the\n",
      "btl_openib_device_param_files MCA parameter to set values for your\n",
      "device.\n",
      "\n",
      "NOTE: You can turn off this warning by setting the MCA parameter\n",
      "      btl_openib_warn_no_device_params_found to 0.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "No OpenFabrics connection schemes reported that they were able to be\n",
      "used on a specific port.  As such, the openib BTL (OpenFabrics\n",
      "support) will be disabled for this port.\n",
      "\n",
      "  Local host:           150-136-218-69\n",
      "  Local device:         mlx5_0\n",
      "  Local port:           1\n",
      "  CPCs attempted:       udcm\n",
      "--------------------------------------------------------------------------\n",
      "/usr/lib/python3/dist-packages/h5py/__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/lib/python3/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "Project config\n",
      "model:\n",
      "  base_learning_rate: 0.0001\n",
      "  target: ldm.models.diffusion.ddpm.LatentDiffusion\n",
      "  params:\n",
      "    linear_start: 0.00085\n",
      "    linear_end: 0.012\n",
      "    num_timesteps_cond: 1\n",
      "    log_every_t: 200\n",
      "    timesteps: 1000\n",
      "    first_stage_key: image\n",
      "    cond_stage_key: txt\n",
      "    image_size: 64\n",
      "    channels: 4\n",
      "    cond_stage_trainable: false\n",
      "    conditioning_key: crossattn\n",
      "    scale_factor: 0.18215\n",
      "    scheduler_config:\n",
      "      target: ldm.lr_scheduler.LambdaLinearScheduler\n",
      "      params:\n",
      "        warm_up_steps:\n",
      "        - 1\n",
      "        cycle_lengths:\n",
      "        - 10000000000000\n",
      "        f_start:\n",
      "        - 1.0e-06\n",
      "        f_max:\n",
      "        - 1.0\n",
      "        f_min:\n",
      "        - 1.0\n",
      "    unet_config:\n",
      "      target: ldm.modules.diffusionmodules.openaimodel.UNetModel\n",
      "      params:\n",
      "        image_size: 32\n",
      "        in_channels: 4\n",
      "        out_channels: 4\n",
      "        model_channels: 320\n",
      "        attention_resolutions:\n",
      "        - 4\n",
      "        - 2\n",
      "        - 1\n",
      "        num_res_blocks: 2\n",
      "        channel_mult:\n",
      "        - 1\n",
      "        - 2\n",
      "        - 4\n",
      "        - 4\n",
      "        num_heads: 8\n",
      "        use_spatial_transformer: true\n",
      "        transformer_depth: 1\n",
      "        context_dim: 768\n",
      "        use_checkpoint: true\n",
      "        legacy: false\n",
      "    first_stage_config:\n",
      "      target: ldm.models.autoencoder.AutoencoderKL\n",
      "      ckpt_path: models/first_stage_models/kl-f8/model.ckpt\n",
      "      params:\n",
      "        embed_dim: 4\n",
      "        monitor: val/rec_loss\n",
      "        ddconfig:\n",
      "          double_z: true\n",
      "          z_channels: 4\n",
      "          resolution: 256\n",
      "          in_channels: 3\n",
      "          out_ch: 3\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 2\n",
      "          - 4\n",
      "          - 4\n",
      "          num_res_blocks: 2\n",
      "          attn_resolutions: []\n",
      "          dropout: 0.0\n",
      "        lossconfig:\n",
      "          target: torch.nn.Identity\n",
      "    cond_stage_config:\n",
      "      target: ldm.modules.encoders.modules.FrozenCLIPEmbedder\n",
      "data:\n",
      "  target: main.DataModuleFromConfig\n",
      "  params:\n",
      "    batch_size: 1\n",
      "    num_workers: 4\n",
      "    num_val_workers: 0\n",
      "    train:\n",
      "      target: ldm.data.simple.hf_dataset_from_local\n",
      "      params:\n",
      "        name: imagefolder\n",
      "        data_dir: res/fontawesome-png\n",
      "        image_transforms:\n",
      "        - target: torchvision.transforms.Resize\n",
      "          params:\n",
      "            size: 512\n",
      "            interpolation: 3\n",
      "        - target: torchvision.transforms.RandomCrop\n",
      "          params:\n",
      "            size: 512\n",
      "        text_column: text\n",
      "        image_column: image\n",
      "        caption_key: text\n",
      "    validation:\n",
      "      target: ldm.data.simple.TextOnly\n",
      "      params:\n",
      "        captions:\n",
      "        - radar\n",
      "        - bunny rabbit\n",
      "        - Yoda\n",
      "        - coffee\n",
      "        - palm tree\n",
      "        - evergreen tree\n",
      "        - toucan\n",
      "        - thumbs up\n",
      "        - butterfly\n",
      "        output_size: 512\n",
      "        n_gpus: 1\n",
      "\n",
      "Lightning config\n",
      "find_unused_parameters: false\n",
      "modelcheckpoint:\n",
      "  params:\n",
      "    every_n_train_steps: 2000\n",
      "    save_top_k: -1\n",
      "    monitor: null\n",
      "callbacks:\n",
      "  image_logger:\n",
      "    target: main.ImageLogger\n",
      "    params:\n",
      "      batch_frequency: 2000\n",
      "      max_images: 4\n",
      "      increase_log_steps: false\n",
      "      log_first_step: true\n",
      "      log_all_val: true\n",
      "      log_images_kwargs:\n",
      "        use_ema_scope: true\n",
      "        inpaint: false\n",
      "        plot_progressive_rows: false\n",
      "        plot_diffusion_rows: false\n",
      "        'N': 4\n",
      "        unconditional_guidance_scale: 3.0\n",
      "        unconditional_guidance_label:\n",
      "        - ''\n",
      "trainer:\n",
      "  benchmark: true\n",
      "  num_sanity_val_steps: 0\n",
      "  accumulate_grad_batches: 1\n",
      "  accelerator: ddp\n",
      "  auto_select_gpus: true\n",
      "  check_val_every_n_epoch: 10\n",
      "  gpus: 1\n",
      "\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | model             | DiffusionWrapper   | 859 M \n",
      "1 | model_ema         | LitEma             | 0     \n",
      "2 | first_stage_model | AutoencoderKL      | 83.7 M\n",
      "3 | cond_stage_model  | FrozenCLIPEmbedder | 123 M \n",
      "---------------------------------------------------------\n",
      "859 M     Trainable params\n",
      "206 M     Non-trainable params\n",
      "1.1 B     Total params\n",
      "4,264.941 Total estimated model params size (MB)\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:322: UserWarning: The number of training samples (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 30 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:   0%|                                  | 0/3 [00:00<00:00, 5146.39it/s]Summoning checkpoint.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 919, in <module>\n",
      "    raise err\n",
      "  File \"main.py\", line 901, in <module>\n",
      "    trainer.fit(model, data)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 553, in fit\n",
      "    self._run(model)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 918, in _run\n",
      "    self._dispatch()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 986, in _dispatch\n",
      "    self.accelerator.start_training(self)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 92, in start_training\n",
      "    self.training_type_plugin.start_training(trainer)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 161, in start_training\n",
      "    self._results = trainer.run_stage()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 996, in run_stage\n",
      "    return self._run_train()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\n",
      "    epoch_output = self.epoch_loop.run(train_dataloader)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 130, in advance\n",
      "    batch_output = self.batch_loop.run(batch, self.iteration_count, self._dataloader_idx)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 101, in run\n",
      "    super().run(batch, batch_idx, dataloader_idx)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 148, in advance\n",
      "    result = self._run_optimization(batch_idx, split_batch, opt_idx, optimizer)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 202, in _run_optimization\n",
      "    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 396, in _optimizer_step\n",
      "    model_ref.optimizer_step(\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1618, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 209, in step\n",
      "    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 129, in __optimizer_step\n",
      "    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 296, in optimizer_step\n",
      "    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 303, in run_optimizer_step\n",
      "    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 226, in optimizer_step\n",
      "    optimizer.step(closure=lambda_closure, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 113, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/optim/adamw.py\", line 119, in step\n",
      "    loss = closure()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 236, in _training_step_and_backward_closure\n",
      "    result = self.training_step_and_backward(split_batch, batch_idx, opt_idx, optimizer, hiddens)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 537, in training_step_and_backward\n",
      "    result = self._training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 307, in _training_step\n",
      "    training_step_output = self.trainer.accelerator.training_step(step_kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 193, in training_step\n",
      "    return self.training_type_plugin.training_step(*step_kwargs.values())\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py\", line 383, in training_step\n",
      "    return self.model(*args, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 1008, in forward\n",
      "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 969, in _run_ddp_forward\n",
      "    return module_to_run(*inputs[0], **kwargs[0])\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\", line 82, in forward\n",
      "    output = self.module.training_step(*inputs, **kwargs)\n",
      "  File \"/home/ubuntu/stable-diffusion/ldm/models/diffusion/ddpm.py\", line 406, in training_step\n",
      "    loss, loss_dict = self.shared_step(batch)\n",
      "  File \"/home/ubuntu/stable-diffusion/ldm/models/diffusion/ddpm.py\", line 872, in shared_step\n",
      "    x, c = self.get_input(batch, self.first_stage_key)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/stable-diffusion/ldm/models/diffusion/ddpm.py\", line 725, in get_input\n",
      "    encoder_posterior = self.encode_first_stage(x)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ubuntu/stable-diffusion/ldm/models/diffusion/ddpm.py\", line 869, in encode_first_stage\n",
      "    return self.first_stage_model.encode(x)\n",
      "  File \"/home/ubuntu/stable-diffusion/ldm/models/autoencoder.py\", line 325, in encode\n",
      "    h = self.encoder(x)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ubuntu/stable-diffusion/ldm/modules/diffusionmodules/model.py\", line 439, in forward\n",
      "    hs = [self.conv_in(x)]\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 457, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 453, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "RuntimeError: Given groups=1, weight of size [128, 3, 3, 3], expected input[1, 4, 512, 512] to have 3 channels, but got 4 channels instead\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "!(python main.py \\\n",
    "    -t \\\n",
    "    --base configs/stable-diffusion/retrain-icons.yaml \\\n",
    "    --auto_select_gpus \\\n",
    "    --gpus=1 \\\n",
    "    --scale_lr False \\\n",
    "    --num_nodes 1 \\\n",
    "    --check_val_every_n_epoch 10 \\\n",
    "    --finetune_from \"$ckpt_path\" \\\n",
    "    data.params.batch_size=\"$BATCH_SIZE\" \\\n",
    "    lightning.trainer.accumulate_grad_batches=1 \\\n",
    "    data.params.validation.params.n_gpus=1 \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fHKEZvS_yVsn",
    "outputId": "e4a543c6-b35b-4624-edbb-dc19fb53c2f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 23\n",
      "Running on GPUs 1\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "Keeping EMAs of 688.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'text_projection.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'logit_scale', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Attempting to load state from /root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v-1-4-original/snapshots/0834a76f88354683d3f7ef271cadd28f4757a8cc/sd-v1-4-full-ema.ckpt\n",
      "/bin/bash: line 1:  1161 Killed                  ( python main.py -t --base configs/stable-diffusion/retrain-icons.yaml --gpus=1 --scale_lr False --num_nodes 1 --check_val_every_n_epoch 10 --finetune_from \"/root/.cache/huggingface/hub/models--CompVis--stable-diffusion-v-1-4-original/snapshots/0834a76f88354683d3f7ef271cadd28f4757a8cc/sd-v1-4-full-ema.ckpt\" data.params.batch_size=\"4\" lightning.trainer.accumulate_grad_batches=1 data.params.validation.params.n_gpus=1 )\n"
     ]
    }
   ],
   "source": [
    "# # Run training\n",
    "# !(python main.py \\\n",
    "#     -t \\\n",
    "#     --base configs/stable-diffusion/retrain-icons.yaml \\\n",
    "#     --gpus=1 \\\n",
    "#     --scale_lr False \\\n",
    "#     --num_nodes 1 \\\n",
    "#     --check_val_every_n_epoch 10 \\\n",
    "#     --finetune_from \"$ckpt_path\" \\\n",
    "#     data.params.batch_size=\"$BATCH_SIZE\" \\\n",
    "#     lightning.trainer.accumulate_grad_batches=1 \\\n",
    "#     data.params.validation.params.n_gpus=1 \\\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TGYZwwfC8A4P",
    "outputId": "84501cc5-8eb3-463b-a9b7-312cbe1e704c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 23\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "Keeping EMAs of 688.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'logit_scale', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'visual_projection.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Monitoring val/loss as checkpoint metric.\n",
      "Merged modelckpt-cfg: \n",
      "{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2022-10-24T13-37-06_retrain-icons/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': None, 'save_top_k': -1, 'every_n_train_steps': 2000}}\n",
      "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:433: UserWarning: ModelCheckpoint(save_last=True, save_top_k=None, monitor=None) is a redundant configuration. You can save the last checkpoint with ModelCheckpoint(save_top_k=None, monitor=None).\n",
      "  \"ModelCheckpoint(save_last=True, save_top_k=None, monitor=None) is a redundant configuration.\"\n",
      "ModelCheckpoint(save_last=True, save_top_k=-1, monitor=None) will duplicate the last checkpoint saved.\n",
      "Namespace(accumulate_grad_batches=None, auto_select_gpus=True, benchmark=True, check_val_every_n_epoch=10, num_sanity_val_steps=0)\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 839, in <module>\n",
      "    trainer = Trainer.from_argparse_args(trainer_opt, **trainer_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/properties.py\", line 421, in from_argparse_args\n",
      "    return from_argparse_args(cls, args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/argparse.py\", line 52, in from_argparse_args\n",
      "    return cls(**trainer_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py\", line 40, in insert_env_defaults\n",
      "    return fn(self, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 446, in __init__\n",
      "    terminate_on_nan,\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/training_trick_connector.py\", line 50, in on_trainer_init\n",
      "    self.configure_accumulated_gradients(accumulate_grad_batches)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/connectors/training_trick_connector.py\", line 66, in configure_accumulated_gradients\n",
      "    raise TypeError(\"Gradient accumulation supports only int and dict types\")\n",
      "TypeError: Gradient accumulation supports only int and dict types\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"main.py\", line 936, in <module>\n",
      "    if trainer.global_rank == 0:\n",
      "NameError: name 'trainer' is not defined\n"
     ]
    }
   ],
   "source": [
    "# # Run training\n",
    "# !(python main.py \\\n",
    "#     -t \\\n",
    "#     --base configs/stable-diffusion/retrain-icons.yaml \\\n",
    "#     --auto_select_gpus \\\n",
    "#     --scale_lr False \\\n",
    "#     --num_nodes 1 \\\n",
    "#     --check_val_every_n_epoch 10 \\\n",
    "#     --finetune_from \"$ckpt_path\" \\\n",
    "#     data.params.batch_size=\"$BATCH_SIZE\" \\\n",
    "#     lightning.trainer.accumulate_grad_batches=\"$ACCUMULATE_BATCHES\" \\\n",
    "#     data.params.validation.params.n_gpus=\"$NUM_GPUS\" \\\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSy2KflDBbbr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "010488febefd4d529784ae4311c9a31c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b6559b90a93416c8511369561a194db",
      "placeholder": "​",
      "style": "IPY_MODEL_f48747d959274a659dcd46a0e6dd0a3d",
      "value": "Generating train split: "
     }
    },
    "010b4e607e1b47299ce1acb046626c31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbddeda33caf456d874de65a8f4bd1f0",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6e7cb796b6d8411d9d7f0e0a833ab00c",
      "value": 0
     }
    },
    "016a0ac954e14f8cb08dc697a9d9f4bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_9df0a9bebfe24df08b82a87bc89cfb8a",
      "placeholder": "​",
      "style": "IPY_MODEL_3596851805c142678d34ad3c6dd9e762",
      "value": ""
     }
    },
    "02507c3b7dfa409ab1ba39fffd36b426": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02708c54eb0e4dd8a32634b720c49a2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "02e7dc6161f54f8eaed207f2070e5229": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99ad6c06e68b486cbc5f89ed47cc0669",
      "max": 7703807346,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_632776253370416da2473354070f017a",
      "value": 7703807346
     }
    },
    "04a14256e3154facbe6f39bccd161559": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cd1f5e4df034a62abb04ccdfc0ac737",
      "placeholder": "​",
      "style": "IPY_MODEL_ba85474177f84bc18bc6cafa700e1199",
      "value": " 7.70G/7.70G [02:02&lt;00:00, 64.1MB/s]"
     }
    },
    "06c00c02914e42459262d60a7dd73a34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9e8a54cc0f34a3ba4e918eecdbb41af",
      "placeholder": "​",
      "style": "IPY_MODEL_ea655bd18fa14b479f2a4981b8c6b930",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "06c1828f3c31435daee428bcc805c799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cbc46aa7f74f491985f4e5abb924fd28",
      "placeholder": "​",
      "style": "IPY_MODEL_a7768c97734d4e49a97d18f3c20b8a34",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "0a2719d9a1c543ffa15182db8b0200c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cd1f5e4df034a62abb04ccdfc0ac737": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d4eade367284cdd9d113638c9ad474c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "1539d9c737b54ebda13a2b7b118fce91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ae6b0e7e281462da09c94cc374db817": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c024e14d3599420d93161add7bbd1710",
      "placeholder": "​",
      "style": "IPY_MODEL_20c9a7ec40174f99a2031d24f3669b7e",
      "value": "Downloading data files: 100%"
     }
    },
    "1f1fbb662039413ca4a9d2117c675f03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a2719d9a1c543ffa15182db8b0200c1",
      "placeholder": "​",
      "style": "IPY_MODEL_286d9cabc07141c28061f35915526e92",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "20432153e5504425a1c3fa8b1d710022": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2097917a39ae406e9e9efaeca325ee6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20c9a7ec40174f99a2031d24f3669b7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "281a6fc232024541ba2bc170ab98391f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "286d9cabc07141c28061f35915526e92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2bb60c219d8a4d28b5278d5beb0f6bd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "300cc2fb8b7c4205be1cab3932819161": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3596851805c142678d34ad3c6dd9e762": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b6559b90a93416c8511369561a194db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f25f5eb265042ab88138831af7df61e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "464a8a73387a4b33a09ea4ac1c451981": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20432153e5504425a1c3fa8b1d710022",
      "placeholder": "​",
      "style": "IPY_MODEL_a2da03b0e3404d4e91a43a28c2958b1e",
      "value": "Downloading: 100%"
     }
    },
    "537aefd8082140468cca39281d09bf8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_06c00c02914e42459262d60a7dd73a34",
       "IPY_MODEL_016a0ac954e14f8cb08dc697a9d9f4bc",
       "IPY_MODEL_7156868013504219a4cb908f2d558d13",
       "IPY_MODEL_06c1828f3c31435daee428bcc805c799"
      ],
      "layout": "IPY_MODEL_ea9ed7901911487eaa3ea1a61020727a"
     }
    },
    "5f5d1cd4b2e94d0b9f12de0c690f8eb0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "632776253370416da2473354070f017a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "64bae32df01b4991998f93ae8e67b757": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9618475279940c181240d507a2d375d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9021a311595f4e2988218a0058730c6b",
      "value": 0
     }
    },
    "6e7cb796b6d8411d9d7f0e0a833ab00c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6ef4cc3bdc414253876b7c192c317821": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1ae6b0e7e281462da09c94cc374db817",
       "IPY_MODEL_9f534add57ec46c892188e50493f68a4",
       "IPY_MODEL_dc89c8da284f434a9fb342e0ae75974c"
      ],
      "layout": "IPY_MODEL_c34caa8a4d854f3795da9c744bddecce"
     }
    },
    "6f7a17bd622842e1b7bcf9f7188abe05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7156868013504219a4cb908f2d558d13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_1539d9c737b54ebda13a2b7b118fce91",
      "style": "IPY_MODEL_02708c54eb0e4dd8a32634b720c49a2d",
      "tooltip": ""
     }
    },
    "7394cd1240824a9096158d9aef264174": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "info",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a264bec25d0641ceb22dcc4052198e87",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3f25f5eb265042ab88138831af7df61e",
      "value": 1
     }
    },
    "765096396ecf47c6aceb46f12556cee9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "772aedd2014c437087170ddfb83b9b19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02507c3b7dfa409ab1ba39fffd36b426",
      "placeholder": "​",
      "style": "IPY_MODEL_778966d743d74f0a90cb6ae480683919",
      "value": "Extracting data files: "
     }
    },
    "778966d743d74f0a90cb6ae480683919": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87a99ca1db6443acb772006d8bb44743": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_765096396ecf47c6aceb46f12556cee9",
      "placeholder": "​",
      "style": "IPY_MODEL_bdb417b6db524af29a600f64fc387df1",
      "value": " 0/0 [00:00&lt;?, ? examples/s]"
     }
    },
    "8b11c88266db4ba88b87903f2e4c7330": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b43934684f24e00b4c829309b4de6a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9021a311595f4e2988218a0058730c6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9480529489394add835edbd5fb8f69fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_772aedd2014c437087170ddfb83b9b19",
       "IPY_MODEL_010b4e607e1b47299ce1acb046626c31",
       "IPY_MODEL_ae94f7cb3e584375988baea3f1f4a9b5"
      ],
      "layout": "IPY_MODEL_8b11c88266db4ba88b87903f2e4c7330"
     }
    },
    "99ad6c06e68b486cbc5f89ed47cc0669": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9df0a9bebfe24df08b82a87bc89cfb8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f534add57ec46c892188e50493f68a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_300cc2fb8b7c4205be1cab3932819161",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2bb60c219d8a4d28b5278d5beb0f6bd8",
      "value": 4
     }
    },
    "a264bec25d0641ceb22dcc4052198e87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "a2da03b0e3404d4e91a43a28c2958b1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7768c97734d4e49a97d18f3c20b8a34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aaa3a1e8efb443969baf60df9cb1aff8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_464a8a73387a4b33a09ea4ac1c451981",
       "IPY_MODEL_02e7dc6161f54f8eaed207f2070e5229",
       "IPY_MODEL_04a14256e3154facbe6f39bccd161559"
      ],
      "layout": "IPY_MODEL_6f7a17bd622842e1b7bcf9f7188abe05"
     }
    },
    "ab3b91591fcb43dcb4f17c9dde29e703": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b43934684f24e00b4c829309b4de6a5",
      "placeholder": "​",
      "style": "IPY_MODEL_c6d1b7331c484f66b5cb123c14630493",
      "value": "Downloading data files: "
     }
    },
    "ac7fa3a7181444cfbe357cde9cbaec91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_010488febefd4d529784ae4311c9a31c",
       "IPY_MODEL_7394cd1240824a9096158d9aef264174",
       "IPY_MODEL_87a99ca1db6443acb772006d8bb44743"
      ],
      "layout": "IPY_MODEL_0d4eade367284cdd9d113638c9ad474c"
     }
    },
    "ae94f7cb3e584375988baea3f1f4a9b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_281a6fc232024541ba2bc170ab98391f",
      "placeholder": "​",
      "style": "IPY_MODEL_ef5892d7a3d34745925966716e07f17f",
      "value": " 0/0 [00:00&lt;?, ?it/s]"
     }
    },
    "ba85474177f84bc18bc6cafa700e1199": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bdb417b6db524af29a600f64fc387df1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c024e14d3599420d93161add7bbd1710": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c210d9364b05491a81fe59ef47b2de22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c34caa8a4d854f3795da9c744bddecce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6d1b7331c484f66b5cb123c14630493": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cbc46aa7f74f491985f4e5abb924fd28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9618475279940c181240d507a2d375d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "d9e8a54cc0f34a3ba4e918eecdbb41af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc89c8da284f434a9fb342e0ae75974c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2097917a39ae406e9e9efaeca325ee6c",
      "placeholder": "​",
      "style": "IPY_MODEL_c210d9364b05491a81fe59ef47b2de22",
      "value": " 4/4 [00:00&lt;00:00, 158.15it/s]"
     }
    },
    "ea655bd18fa14b479f2a4981b8c6b930": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea9ed7901911487eaa3ea1a61020727a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "ef5892d7a3d34745925966716e07f17f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f48747d959274a659dcd46a0e6dd0a3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f65c0c8fcb6d4801b30498bf870480de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ab3b91591fcb43dcb4f17c9dde29e703",
       "IPY_MODEL_64bae32df01b4991998f93ae8e67b757",
       "IPY_MODEL_1f1fbb662039413ca4a9d2117c675f03"
      ],
      "layout": "IPY_MODEL_5f5d1cd4b2e94d0b9f12de0c690f8eb0"
     }
    },
    "fbddeda33caf456d874de65a8f4bd1f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
